"""
Sistema de Retroalimentaci√≥n con GPT
Genera feedback autom√°tico comparando documentos con r√∫bricas
"""
from openai import OpenAI
import json
import os
from typing import Dict, List
from dotenv import load_dotenv

load_dotenv()

class GPTFeedbackGenerator:
    """Genera retroalimentaci√≥n acad√©mica usando GPT-4"""

    def __init__(self):
        """Inicializa el cliente de OpenAI"""
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.client = OpenAI(api_key=self.openai_api_key)
        self.model = "gpt-4o-mini"  # Opciones: gpt-4o-mini (barato), gpt-4o (mejor calidad)

    def generate_criterion_feedback(self, criterion: Dict, document_content: str,
                                    course_name: str, detected_criterion: int = None,
                                    exercises_in_document: list = None) -> Dict:
        """
        Genera retroalimentaci√≥n para un criterio espec√≠fico (NUEVA ESTRUCTURA)
        ACTUALIZADO: Primero verifica si el criterio est√° presente en el documento

        Args:
            criterion: Dict con estructura del criterio (numero, nombre, niveles, etc.)
            document_content: Contenido del documento del estudiante
            course_name: Nombre del curso
            detected_criterion: N√∫mero del criterio detectado desde el nombre del archivo (opcional)
            exercises_in_document: Lista de ejercicios detectados en el documento (opcional)

        Returns:
            Dict con feedback, puntaje y nivel alcanzado (o no_presentado si no aplica)
        """
        try:
            # Extraer informaci√≥n del criterio
            criterion_number = criterion['numero']
            criterion_name = criterion['nombre']
            max_score = criterion['puntaje_maximo']
            levels = criterion.get('niveles', [])

            # Usar ejercicios pasados o detectarlos
            if exercises_in_document is None:
                exercises_in_doc = self._detect_exercises_in_document(document_content)
            else:
                exercises_in_doc = exercises_in_document

            print(f"  üìù Evaluando con ejercicios detectados: {exercises_in_doc}")

            # NUEVA VALIDACI√ìN: Verificar si el criterio est√° presente en el documento
            # El nombre del archivo es solo una PISTA, NO es definitivo
            is_present = self._is_criterion_present(
                criterion,
                document_content,
                detected_criterion  # Pasar como pista
            )

            if not is_present:
                return {
                    'success': True,
                    'criterion_number': criterion_number,
                    'criterion_name': criterion_name,
                    'max_score': max_score,
                    'score': 0,
                    'level_achieved': 'no_presentado',
                    'feedback': f'No se encontr√≥ evidencia de este criterio en el documento. El estudiante no present√≥ trabajo relacionado con: {criterion_name}.',
                    'aspects_met': [],
                    'improvements': [
                        f'Incluir evidencia clara de {criterion_name.lower()}',
                        'Asegurarse de cumplir con todos los requisitos de la r√∫brica'
                    ]
                }

            # Construir texto de niveles
            levels_text = ""
            for level in levels:
                levels_text += f"\n{level['nivel'].upper()} ({level['puntaje_minimo']}-{level['puntaje_maximo']} pts): {level['descripcion'][:200]}"

            # Informaci√≥n de ejercicios detectados
            exercises_info = ""
            if len(exercises_in_doc) > 0:
                exercises_info = f"\n\n‚ö†Ô∏è EJERCICIOS DETECTADOS EN EL DOCUMENTO: {exercises_in_doc}\nEsto significa que el estudiante menciona expl√≠citamente estos ejercicios."

            # Construir prompt para GPT
            prompt = f"""
Eres un profesor experto y motivador en {course_name}. Eval√∫a el siguiente criterio de un trabajo estudiantil con un tono cercano, profesional y constructivo.

CRITERIO {criterion_number}: {criterion_name}
Puntaje m√°ximo: {max_score} puntos

NIVELES DE DESEMPE√ëO:
{levels_text}

CONTENIDO DEL DOCUMENTO:
{document_content[:4000]}
{exercises_info}

INSTRUCCIONES PARA GENERAR FEEDBACK:

1. **Tono y Estilo**:
   - Usa un tono cercano y motivador (ej: "Excelente trabajo", "Tu implementaci√≥n demuestra...", "Se observa que...")
   - S√© espec√≠fico con los datasets, m√©tricas y t√©cnicas que us√≥ el estudiante
   - Menciona IDs de datasets si los encuentras (ej: "liver-disorders (ID:8)")
   - Reconoce los logros primero, luego sugiere mejoras

2. **Detecci√≥n de Ejercicios**:
   - Busca menciones literales: "Ejercicio 1", "Ejercicio 2", "Ejercicio 3", etc.
   - Si solo present√≥ ALGUNOS ejercicios ‚Üí Puntaje PROPORCIONAL
   - Menciona EXACTAMENTE cu√°les ejercicios present√≥

3. **Estructura del Feedback** (seg√∫n el criterio):

   **Para Criterio 1 (Carga y contextualizaci√≥n)**:
   - Menciona si explic√≥ el prop√≥sito m√©dico/cient√≠fico de los datasets
   - Verifica si identific√≥ correctamente variables objetivo y predictoras
   - Revisa si especific√≥ tama√±os de datasets

   **Para Criterio 2 (Regresi√≥n)**:
   - Menciona qu√© modelos implement√≥ (Lineal, Ridge, Lasso, √Årbol)
   - Verifica divisi√≥n de datos (75%-25%)
   - Revisa c√°lculo de m√©tricas (MAE, MSE, RMSE, R¬≤)
   - Menciona si compar√≥ modelos en tabla

   **Para Criterio 3 (Clasificaci√≥n)**:
   - Menciona qu√© modelos implement√≥ (Regresi√≥n Log√≠stica, √Årbol, KNN, Perceptr√≥n)
   - Verifica divisi√≥n de datos (70%-30%)
   - Revisa c√°lculo de m√©tricas (Accuracy, Precision, Recall, F1-score)
   - Verifica matriz de confusi√≥n

   **Para Criterio 4 (Foro)**:
   - Menciona si adjunt√≥ screenshot del foro
   - Eval√∫a calidad del feedback (constructivo, respetuoso, argumentado)

   **Para Criterio 5 (Formato)**:
   - Eval√∫a estructura, organizaci√≥n, claridad

4. **Ejemplos de Feedback Esperado**:
   - "Excelente trabajo en el Ejercicio X, cumples completamente con todos los requisitos solicitados..."
   - "Tu trabajo demuestra dominio t√©cnico en la implementaci√≥n de los cuatro modelos..."
   - "El estudiante evidenci√≥ su compromiso con la din√°mica del Ejercicio 5..."

FORMATO DE RESPUESTA (JSON):
{{
  "nivel_alcanzado": "<alto/medio/bajo>",
  "puntaje": <n√∫mero entre 0 y {max_score}>,
  "feedback": "<feedback detallado, motivador y espec√≠fico (2-4 p√°rrafos)>",
  "aspectos_cumplidos": ["<aspecto espec√≠fico 1>", "<aspecto espec√≠fico 2>", "<aspecto espec√≠fico 3>"],
  "mejoras": ["<sugerencia constructiva 1>", "<sugerencia constructiva 2>"]
}}
"""

            # Llamar a GPT
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Eres un profesor universitario experto, cercano y motivador. Proporcionas retroalimentaci√≥n detallada, espec√≠fica y constructiva que reconoce logros y gu√≠a mejoras."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4,
                max_tokens=900,
                response_format={"type": "json_object"}
            )

            # Parsear respuesta
            feedback_data = json.loads(response.choices[0].message.content)

            return {
                'success': True,
                'criterion_number': criterion_number,
                'criterion_name': criterion_name,
                'max_score': max_score,
                'score': feedback_data.get('puntaje', 0),
                'level_achieved': feedback_data.get('nivel_alcanzado', 'medio'),
                'feedback': feedback_data.get('feedback', ''),
                'aspects_met': feedback_data.get('aspectos_cumplidos', []),
                'improvements': feedback_data.get('mejoras', [])
            }

        except Exception as e:
            print(f"‚úó Error generando feedback para criterio '{criterion_name}': {e}")
            return {
                'success': False,
                'criterion_number': criterion.get('numero', 0),
                'criterion_name': criterion.get('nombre', ''),
                'error': str(e)
            }

    def generate_overall_feedback_criteria(self, course_name: str, criteria_feedbacks: List[Dict],
                                          total_score: float, max_score: int) -> Dict:
        """
        Genera retroalimentaci√≥n general usando NUEVA estructura de criterios

        Args:
            course_name: Nombre del curso
            criteria_feedbacks: Lista de feedbacks por criterio
            total_score: Puntaje total obtenido
            max_score: Puntaje m√°ximo posible

        Returns:
            Dict con feedback general y recomendaciones
        """
        try:
            # Resumir resultados por criterio
            criteria_summary = []
            for fb in criteria_feedbacks:
                if fb.get('success'):
                    criteria_summary.append(
                        f"- Criterio {fb['criterion_number']}: {fb['score']}/{fb['max_score']} pts ({fb['level_achieved']})"
                    )

            summary_text = '\n'.join(criteria_summary)
            percentage = (total_score / max_score * 100) if max_score > 0 else 0

            prompt = f"""
Eres un profesor de {course_name}. Proporciona una retroalimentaci√≥n GENERAL sobre un trabajo estudiantil.

PUNTAJE FINAL: {total_score}/{max_score} ({percentage:.1f}%)

RESULTADOS POR CRITERIO:
{summary_text}

INSTRUCCIONES:
1. Resume el desempe√±o general del estudiante (m√°ximo 2 p√°rrafos cortos)
2. Destaca 2-3 fortalezas principales
3. Indica 2-3 √°reas de mejora prioritarias
4. Proporciona una conclusi√≥n motivadora

FORMATO DE RESPUESTA (JSON):
{{
  "resumen": "<resumen general>",
  "fortalezas": ["<fortaleza 1>", "<fortaleza 2>"],
  "areas_mejora": ["<√°rea 1>", "<√°rea 2>"],
  "conclusion": "<conclusi√≥n motivadora>"
}}
"""

            # Llamar a GPT
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Eres un profesor universitario que proporciona retroalimentaci√≥n motivadora y constructiva."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4,
                max_tokens=600,
                response_format={"type": "json_object"}
            )

            # Parsear respuesta
            overall_data = json.loads(response.choices[0].message.content)

            return {
                'success': True,
                'total_score': total_score,
                'max_score': max_score,
                'percentage': round(percentage, 1),
                'summary': overall_data.get('resumen', ''),
                'strengths': overall_data.get('fortalezas', []),
                'improvement_areas': overall_data.get('areas_mejora', []),
                'conclusion': overall_data.get('conclusion', '')
            }

        except Exception as e:
            print(f"‚úó Error generando feedback general: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def generate_section_feedback(self, section_name: str, section_criteria: List[str],
                                  section_weight: int, document_content: str,
                                  course_name: str) -> Dict:
        """
        Genera retroalimentaci√≥n para una secci√≥n espec√≠fica

        Args:
            section_name: Nombre de la secci√≥n
            section_criteria: Lista de criterios de evaluaci√≥n
            section_weight: Peso de la secci√≥n (%)
            document_content: Contenido del documento del estudiante
            course_name: Nombre del curso

        Returns:
            Dict con feedback, puntaje y recomendaciones
        """
        try:
            # Construir prompt para GPT
            criteria_text = '\n'.join([f"{i+1}. {c}" for i, c in enumerate(section_criteria)])

            prompt = f"""
Eres un profesor experto en {course_name}. Eval√∫a la secci√≥n "{section_name}" de un trabajo estudiantil.

CRITERIOS DE EVALUACI√ìN (Peso: {section_weight}%):
{criteria_text}

CONTENIDO DEL DOCUMENTO:
{document_content[:3000]}

INSTRUCCIONES:
1. Eval√∫a qu√© criterios se cumplen y cu√°les no
2. Asigna un puntaje de 0-100 para esta secci√≥n
3. Proporciona retroalimentaci√≥n CONCISA (m√°ximo 3 p√°rrafos cortos)
4. Sugiere 2-3 mejoras espec√≠ficas

FORMATO DE RESPUESTA (JSON):
{{
  "puntaje": <n√∫mero 0-100>,
  "cumple_criterios": [<lista de criterios cumplidos>],
  "no_cumple_criterios": [<lista de criterios no cumplidos>],
  "feedback": "<retroalimentaci√≥n concisa>",
  "mejoras": ["<mejora 1>", "<mejora 2>", "<mejora 3>"]
}}
"""

            # Llamar a GPT
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Eres un profesor universitario experto que proporciona retroalimentaci√≥n constructiva y concisa."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=800,
                response_format={"type": "json_object"}
            )

            # Parsear respuesta
            feedback_data = json.loads(response.choices[0].message.content)

            return {
                'success': True,
                'section': section_name,
                'weight': section_weight,
                'score': feedback_data.get('puntaje', 0),
                'criteria_met': feedback_data.get('cumple_criterios', []),
                'criteria_not_met': feedback_data.get('no_cumple_criterios', []),
                'feedback': feedback_data.get('feedback', ''),
                'improvements': feedback_data.get('mejoras', [])
            }

        except Exception as e:
            print(f"‚úó Error generando feedback para secci√≥n '{section_name}': {e}")
            return {
                'success': False,
                'section': section_name,
                'error': str(e)
            }

    def generate_overall_feedback(self, course_name: str, section_feedbacks: List[Dict],
                                  total_score: float) -> Dict:
        """
        Genera retroalimentaci√≥n general del documento

        Args:
            course_name: Nombre del curso
            section_feedbacks: Lista de feedbacks por secci√≥n
            total_score: Puntaje total ponderado

        Returns:
            Dict con feedback general y recomendaciones
        """
        try:
            # Resumir resultados por secci√≥n
            sections_summary = []
            for fb in section_feedbacks:
                if fb.get('success'):
                    sections_summary.append(
                        f"- {fb['section']}: {fb['score']}/100 (Peso: {fb['weight']}%)"
                    )

            summary_text = '\n'.join(sections_summary)

            prompt = f"""
Eres un profesor de {course_name}. Proporciona una retroalimentaci√≥n GENERAL sobre un trabajo estudiantil.

PUNTAJE FINAL: {total_score:.1f}/100

RESULTADOS POR SECCI√ìN:
{summary_text}

INSTRUCCIONES:
1. Resume el desempe√±o general del estudiante (m√°ximo 2 p√°rrafos cortos)
2. Destaca 2-3 fortalezas principales
3. Indica 2-3 √°reas de mejora prioritarias
4. Proporciona una conclusi√≥n motivadora

FORMATO DE RESPUESTA (JSON):
{{
  "resumen": "<resumen general>",
  "fortalezas": ["<fortaleza 1>", "<fortaleza 2>"],
  "areas_mejora": ["<√°rea 1>", "<√°rea 2>"],
  "conclusion": "<conclusi√≥n motivadora>"
}}
"""

            # Llamar a GPT
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Eres un profesor universitario que proporciona retroalimentaci√≥n motivadora y constructiva."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.4,
                max_tokens=600,
                response_format={"type": "json_object"}
            )

            # Parsear respuesta
            overall_data = json.loads(response.choices[0].message.content)

            return {
                'success': True,
                'total_score': total_score,
                'summary': overall_data.get('resumen', ''),
                'strengths': overall_data.get('fortalezas', []),
                'improvement_areas': overall_data.get('areas_mejora', []),
                'conclusion': overall_data.get('conclusion', '')
            }

        except Exception as e:
            print(f"‚úó Error generando feedback general: {e}")
            return {
                'success': False,
                'error': str(e)
            }

    def evaluate_document(self, document_content: str, rubric_data: Dict,
                         relevant_sections: List[Dict] = None, file_name: str = None) -> Dict:
        """
        Eval√∫a un documento completo contra una r√∫brica
        SOPORTA NUEVA ESTRUCTURA: criterios_evaluacion

        Args:
            document_content: Contenido extra√≠do del documento
            rubric_data: Datos de la r√∫brica del curso
            relevant_sections: Secciones relevantes encontradas por Pinecone (opcional)
            file_name: Nombre del archivo subido (para detectar criterio) (opcional)

        Returns:
            Dict con evaluaci√≥n completa
        """
        course_name = rubric_data['nombre_curso']

        # NUEVA ESTRUCTURA: criterios_evaluacion (desde PDF)
        if 'criterios_evaluacion' in rubric_data:
            return self._evaluate_with_criteria(document_content, rubric_data, relevant_sections, file_name)

        # ESTRUCTURA ANTIGUA: condiciones_entrega (compatibilidad)
        elif 'condiciones_entrega' in rubric_data:
            return self._evaluate_with_sections(document_content, rubric_data, relevant_sections)

        else:
            return {
                'success': False,
                'error': 'Estructura de r√∫brica no reconocida'
            }

    def _evaluate_with_criteria(self, document_content: str, rubric_data: Dict,
                                relevant_sections: List[Dict] = None, file_name: str = None) -> Dict:
        """Eval√∫a documento usando NUEVA estructura de criterios"""
        course_name = rubric_data['nombre_curso']
        criteria_to_evaluate = rubric_data['criterios_evaluacion']

        print(f"\n[EVAL] Evaluando documento para: {course_name}")
        print(f"       Archivo: {file_name if file_name else 'Sin nombre'}")
        print(f"       Total criterios: {len(criteria_to_evaluate)}")

        # PRIMERO: Detectar ejercicios en el documento
        exercises_in_doc = self._detect_exercises_in_document(document_content)
        print(f"       üîç Ejercicios detectados en documento: {exercises_in_doc if exercises_in_doc else 'Ninguno'}")

        # Detectar criterio/ejercicio desde nombre del archivo
        detected_criterion = self._detect_criterion_from_filename(file_name) if file_name else None

        # Tambi√©n buscar "Ejercicio X" en el nombre del archivo
        # IGNORAR n√∫meros entre par√©ntesis como (2), (1), etc.
        if file_name and not detected_criterion:
            import re
            # Buscar "ejercicio X", "tarea X", etc. pero NO n√∫meros entre par√©ntesis
            # Primero eliminar n√∫meros entre par√©ntesis del nombre
            clean_name = re.sub(r'\(\d+\)', '', file_name)  # Elimina (1), (2), etc.

            match = re.search(r'ejercicio\s*(\d+)', clean_name.lower())
            if match:
                detected_criterion = int(match.group(1))
                print(f"       ‚úì Ejercicio detectado desde nombre archivo: {detected_criterion}")

        if detected_criterion:
            print(f"       ‚úì Criterio/Ejercicio detectado desde nombre: {detected_criterion}")
            print(f"       ‚ö†Ô∏è MODO FILTRADO: Solo se evaluar√° el Criterio {detected_criterion}")

        # Evaluar cada criterio
        criteria_feedbacks = []
        total_score = 0
        total_max_score = rubric_data.get('puntaje_total', 150)

        for i, criterion in enumerate(criteria_to_evaluate, 1):
            criterion_num = criterion['numero']
            print(f"  [{i}/{len(criteria_to_evaluate)}] Evaluando Criterio {criterion_num}: {criterion['nombre']}...")

            # FILTRO: Si el nombre del archivo indica un criterio espec√≠fico
            # Solo evaluar ese criterio (excepto 4 y 5 que siempre se eval√∫an)
            if detected_criterion is not None:
                if criterion_num not in [4, 5] and criterion_num != detected_criterion:
                    print(f"  ‚è≠Ô∏è Criterio {criterion_num}: SALTADO (archivo indica Criterio {detected_criterion})")
                    # Crear feedback de NO PRESENTADO
                    feedback = {
                        'success': True,
                        'criterion_number': criterion_num,
                        'criterion_name': criterion['nombre'],
                        'max_score': criterion['puntaje_maximo'],
                        'score': 0,
                        'level_achieved': 'no_presentado',
                        'feedback': f'El nombre del archivo indica que este documento corresponde al Criterio/Ejercicio {detected_criterion}, no al Criterio {criterion_num}.',
                        'aspects_met': [],
                        'improvements': [f'Subir documento espec√≠fico para el Criterio {criterion_num}']
                    }
                    criteria_feedbacks.append(feedback)
                    continue

            # Generar feedback para este criterio
            feedback = self.generate_criterion_feedback(
                criterion=criterion,
                document_content=document_content,
                course_name=course_name,
                detected_criterion=detected_criterion,  # NUEVO
                exercises_in_document=exercises_in_doc  # NUEVO
            )

            if feedback.get('success'):
                criteria_feedbacks.append(feedback)
                total_score += feedback['score']
            else:
                print(f"  ‚úó Error en criterio: {criterion['nombre']}")

        # Generar retroalimentaci√≥n general
        print(f"\n  [GENERAL] Generando feedback general...")
        overall_feedback = self.generate_overall_feedback_criteria(
            course_name=course_name,
            criteria_feedbacks=criteria_feedbacks,
            total_score=total_score,
            max_score=total_max_score
        )

        return {
            'success': True,
            'course': course_name,
            'total_score': round(total_score, 1),
            'max_score': total_max_score,
            'criteria_feedbacks': criteria_feedbacks,
            'overall_feedback': overall_feedback,
            'timestamp': self._get_timestamp()
        }

    def _evaluate_with_sections(self, document_content: str, rubric_data: Dict,
                               relevant_sections: List[Dict] = None) -> Dict:
        """Eval√∫a documento usando ESTRUCTURA ANTIGUA de secciones"""
        course_name = rubric_data['nombre_curso']
        sections_to_evaluate = rubric_data['condiciones_entrega']

        # Si hay secciones relevantes de Pinecone, priorizarlas
        if relevant_sections:
            print(f"[INFO] Priorizando {len(relevant_sections)} secciones relevantes")
            # Ordenar por relevancia
            relevant_sections.sort(key=lambda x: x['relevance_score'], reverse=True)

        print(f"\n[EVAL] Evaluando documento para: {course_name}")
        print(f"       Total secciones: {len(sections_to_evaluate)}")

        # Evaluar cada secci√≥n
        section_feedbacks = []
        total_weighted_score = 0

        for i, section in enumerate(sections_to_evaluate, 1):
            print(f"  [{i}/{len(sections_to_evaluate)}] Evaluando: {section['seccion']}...")

            feedback = self.generate_section_feedback(
                section_name=section['seccion'],
                section_criteria=section['criterios'],
                section_weight=section['peso'],
                document_content=document_content,
                course_name=course_name
            )

            if feedback.get('success'):
                section_feedbacks.append(feedback)
                # Calcular puntaje ponderado
                weighted_score = (feedback['score'] / 100) * section['peso']
                total_weighted_score += weighted_score
            else:
                print(f"  ‚úó Error en secci√≥n: {section['seccion']}")

        # Generar retroalimentaci√≥n general
        print(f"\n  [GENERAL] Generando feedback general...")
        overall_feedback = self.generate_overall_feedback(
            course_name=course_name,
            section_feedbacks=section_feedbacks,
            total_score=total_weighted_score
        )

        return {
            'success': True,
            'course': course_name,
            'total_score': round(total_weighted_score, 1),
            'section_feedbacks': section_feedbacks,
            'overall_feedback': overall_feedback,
            'timestamp': self._get_timestamp()
        }

    def _detect_exercises_in_document(self, document_content: str) -> list:
        """
        Detecta qu√© ejercicios est√°n presentes en el documento buscando literalmente
        "Ejercicio 1", "Ejercicio 2", etc.

        VERSI√ìN MEJORADA: Maneja saltos de l√≠nea y espacios del OCR

        Returns:
            Lista de n√∫meros de ejercicios encontrados (ej: [1, 2, 5])
        """
        import re

        doc_lower = document_content.lower()
        exercises_found = []

        # Patrones FLEXIBLES para buscar ejercicios (permiten saltos de l√≠nea y m√∫ltiples espacios)
        patterns = [
            r'ejercicio[\s\n\r]*(\d+)',      # Ejercicio 1, Ejercicio\n1, etc.
            r'exercise[\s\n\r]*(\d+)',
            r'actividad[\s\n\r]*(\d+)',
            r'activity[\s\n\r]*(\d+)',
            r'punto[\s\n\r]*(\d+)',
            r'item[\s\n\r]*(\d+)',
            r'tarea[\s\n\r]*(\d+)',
            r'task[\s\n\r]*(\d+)',
        ]

        for pattern in patterns:
            matches = re.finditer(pattern, doc_lower, re.MULTILINE)
            for match in matches:
                exercise_num = int(match.group(1))
                if exercise_num not in exercises_found and 1 <= exercise_num <= 10:
                    exercises_found.append(exercise_num)

        return sorted(exercises_found)

    def _is_criterion_present(self, criterion: Dict, document_content: str, detected_criterion: int = None) -> bool:
        """
        Verifica si un criterio espec√≠fico est√° presente en el documento usando GPT
        VERSI√ìN BALANCEADA: Usa keywords + GPT, con el nombre del archivo como PISTA

        Args:
            criterion: Dict con informaci√≥n del criterio
            document_content: Contenido del documento
            detected_criterion: Criterio detectado desde nombre archivo (PISTA, no absoluto)

        Returns:
            True si el criterio est√° presente, False si no
        """
        try:
            criterion_name = criterion['nombre']
            criterion_num = criterion.get('numero', 0)

            # PISTA POSITIVA: Si el nombre del archivo indica ESTE criterio ‚Üí Facilitar detecci√≥n
            file_hint_matches = (detected_criterion is not None and detected_criterion == criterion_num)

            # Detectar ejercicios presentes en el documento
            exercises_in_doc = self._detect_exercises_in_document(document_content)

            print(f"\n  [DEBUG] Criterio {criterion_num}:")
            print(f"    - detected_criterion: {detected_criterion}")
            print(f"    - file_hint_matches: {file_hint_matches}")
            print(f"    - exercises_found: {exercises_in_doc}")

            if file_hint_matches:
                print(f"  üí° Criterio {criterion_num}: Nombre del archivo indica este criterio (PISTA POSITIVA)")

            # DETECCI√ìN DIRECTA POR EJERCICIOS
            # Si encuentra "Ejercicio X" en el documento, asumir que el criterio est√° presente
            if len(exercises_in_doc) > 0:
                # Si este criterio est√° en la lista de ejercicios detectados, PRESENTE
                if criterion_num in exercises_in_doc:
                    print(f"  ‚úÖ Criterio {criterion_num}: Encontr√≥ Ejercicio {criterion_num} en el documento ‚Üí PRESENTE")
                    return True
                else:
                    print(f"  ‚ö†Ô∏è Criterio {criterion_num}: Ejercicios detectados {exercises_in_doc}, pero no incluye {criterion_num}")

            # Keywords OBLIGATORIAS ESPEC√çFICAS (m√°s flexibles ahora)
            # Se aceptan keywords generales que indiquen presencia del criterio
            required_keywords = {
                1: [
                    ['dataset', 'datos', 'data', 'csv', 'archivo'],
                    ['carga', 'load', 'read_csv', 'lectura'],
                    ['an√°lisis', 'exploraci√≥n', 'EDA', 'describe', 'info', 'head'],
                ],
                2: [
                    ['regresi√≥n', 'regression', 'regressor', 'predic'],
                    ['MAE', 'MSE', 'RMSE', 'R¬≤', 'r2', 'error', 'm√©trica'],
                ],
                3: [
                    ['clasificaci√≥n', 'classification', 'classifier', 'clase'],
                    ['accuracy', 'precision', 'recall', 'F1', 'score', 'exactitud'],
                ],
                4: [
                    ['foro', 'forum', 'participaci√≥n', 'comentario'],
                ],
                5: [
                    # Criterio 5 siempre presente (formato)
                    ['documento', 'entrega', 'formato', 'archivo']
                ]
            }

            # Keywords DE EXCLUSI√ìN (si est√°n presentes, DESCARTAR el criterio)
            exclusion_keywords = {
                1: ['regresi√≥n', 'regression', 'clasificaci√≥n', 'classification', 'MAE', 'MSE', 'accuracy', 'precision', 'recall'],  # Si tiene modelos ‚Üí NO es solo Criterio 1
                2: ['clasificaci√≥n', 'classification', 'accuracy', 'precision', 'recall', 'F1'],  # Si tiene clasificaci√≥n ‚Üí NO es Criterio 2
                3: ['regresi√≥n', 'regression', 'MAE', 'MSE', 'RMSE']  # Si SOLO tiene regresi√≥n ‚Üí NO es Criterio 3
            }

            doc_lower = document_content.lower()

            # FASE 0: DESHABILITADA - No usar exclusiones autom√°ticas
            # Permitir que GPT decida basado en el contenido completo
            # (Las exclusiones eran demasiado agresivas para trabajos completos)

            # FASE 1: Verificaci√≥n r√°pida de keywords obligatorias
            required_groups = required_keywords.get(criterion_num, [])
            groups_matched = 0

            for group in required_groups:
                if any(kw.lower() in doc_lower for kw in group):
                    groups_matched += 1

            # Ajustar requisitos seg√∫n pista de archivo
            # AHORA M√ÅS FLEXIBLE: Solo necesita 1 grupo en general
            if file_hint_matches:
                min_groups = 1  # Con pista: 1 grupo
            else:
                # Sin pista: Tambi√©n 1 grupo (MUY FLEXIBLE para permitir trabajos completos)
                min_groups = 1

            if groups_matched < min_groups:
                print(f"  ‚úó Criterio {criterion_num}: Solo {groups_matched}/{len(required_groups)} grupos obligatorios ‚Üí NO PRESENTADO")
                return False

            print(f"  ‚úì Criterio {criterion_num}: Encontr√≥ {groups_matched}/{len(required_groups)} grupos de keywords (m√≠nimo: {min_groups})")

            # FASE 2: Validaci√≥n con GPT
            # Si NO hay criterio detectado desde archivo, ser MUY PERMISIVO (trabajo completo)
            if detected_criterion is None:
                strictness = "PERMISIVO: Da el beneficio de la duda. Si hay CUALQUIER evidencia m√≠nima del criterio, marca como PRESENTE."
            else:
                strictness = "BALANCEADO: Busca evidencia razonable del criterio."

            prompt = f"""
Eres un evaluador acad√©mico {strictness}

Determina si el siguiente documento contiene evidencia del criterio:

CRITERIO {criterion_num}: "{criterion_name}"

DOCUMENTO COMPLETO:
{document_content[:4000]}

INSTRUCCIONES:

**Criterio 1** (Carga y an√°lisis de datos):
- Busca: Carga de dataset, an√°lisis, limpieza, exploraci√≥n
- Si encuentra estas actividades ‚Üí TRUE

**Criterio 2** (Modelos de REGRESI√ìN):
- Busca: Implementaci√≥n de regresi√≥n, m√©tricas (MAE, MSE, RMSE, R¬≤)
- Si encuentra modelos de regresi√≥n ‚Üí TRUE

**Criterio 3** (Modelos de CLASIFICACI√ìN):
- Busca: Implementaci√≥n de clasificaci√≥n, m√©tricas (accuracy, precision, recall, F1)
- Si encuentra modelos de clasificaci√≥n ‚Üí TRUE

**Criterio 4** (Participaci√≥n en foro):
- Busca: Menciones de foro, retroalimentaci√≥n, participaci√≥n
- Si encuentra participaci√≥n ‚Üí TRUE

**Criterio 5** (Formato):
- Siempre TRUE (eval√∫a formato del documento)

IMPORTANTE: Si encuentras evidencia razonable del criterio, marca como TRUE.
No seas demasiado estricto. Si hay dudas, da el beneficio de la duda al estudiante.

Responde SOLO con JSON:
{{
  "presente": true/false,
  "razon": "<explicaci√≥n breve>",
  "confianza": "<alta/media/baja>"
}}
"""
            # Sistema de evaluaci√≥n seg√∫n contexto
            if detected_criterion is None:
                system_msg = "Eres un evaluador acad√©mico MUY PERMISIVO. El estudiante present√≥ un trabajo completo. Marca 'presente: true' si encuentras CUALQUIER evidencia del criterio, por m√≠nima que sea."
            else:
                system_msg = "Eres un evaluador acad√©mico JUSTO. Marca 'presente: true' si hay evidencia razonable del criterio."

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,  # M√°s determin√≠stico
                max_tokens=200,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            is_present = result.get('presente', False)
            confidence = result.get('confianza', 'baja')
            reason = result.get('razon', '')

            # DECISI√ìN FINAL: Combinar keywords + GPT + pista de archivo
            # Si GPT dice que NO est√° presente, rechazar inmediatamente
            if not is_present:
                # PERO: Si el nombre del archivo coincide, dar una segunda oportunidad
                if file_hint_matches:
                    print(f"  ‚ö† Criterio {criterion_num}: GPT dice NO PRESENTE pero archivo indica este criterio")
                    print(f"     ‚Üí ACEPTAR por pista de archivo (raz√≥n GPT: {reason[:80]})")
                    return True
                else:
                    print(f"  ‚úó Criterio {criterion_num}: GPT confirm√≥ NO PRESENTE ‚Üí {reason}")
                    return False

            # Si GPT dice S√ç pero con confianza BAJA
            if is_present and confidence == 'baja':
                # Si hay pista de archivo, ACEPTAR igual
                if file_hint_matches:
                    print(f"  ‚úì Criterio {criterion_num}: Confianza baja pero archivo coincide ‚Üí ACEPTAR")
                    return True
                else:
                    print(f"  ‚ö† Criterio {criterion_num}: GPT dice PRESENTE pero confianza BAJA ‚Üí NO PRESENTADO ({reason})")
                    return False

            # Si lleg√≥ aqu√≠: GPT confirm√≥ con confianza media/alta
            print(f"  ‚úì Criterio {criterion_num}: PRESENTE confirmado (grupos: {groups_matched}, confianza: {confidence})")
            print(f"     Raz√≥n: {reason[:100]}")
            return True

        except Exception as e:
            print(f"‚ö† Error verificando presencia del criterio: {e}")
            # En caso de error, RECHAZAR por defecto (modo estricto)
            return False

    def _detect_criterion_from_filename(self, file_name: str) -> int:
        """
        Detecta el n√∫mero de criterio/tarea desde el nombre del archivo
        Ignora n√∫meros entre par√©ntesis como (1), (2), etc.

        Args:
            file_name: Nombre del archivo (ej: "tarea_2.pdf", "criterio3.ipynb", "ejercicio_1.png")

        Returns:
            N√∫mero del criterio detectado (1-5) o None si no se detecta
        """
        import re

        if not file_name:
            return None

        # Eliminar n√∫meros entre par√©ntesis primero (para evitar confusi√≥n con versiones)
        file_lower = re.sub(r'\(\d+\)', '', file_name.lower())

        # Patrones para detectar criterio/tarea/ejercicio
        # IMPORTANTE: Usar \b (word boundary) para evitar coincidencias dentro de palabras como "Fase2"
        patterns = [
            r'criterio[_\s-]?(\d)',
            r'tarea[_\s-]?(\d)',
            r'ejercicio[_\s-]?(\d)',
            r'actividad[_\s-]?(\d)',
            r'punto[_\s-]?(\d)',
            r'task[_\s-]?(\d)',
            r'criterion[_\s-]?(\d)',
            r'activity[_\s-]?(\d)',
            r'\bc(\d)',  # c1, c2, c3 (solo al inicio de palabra)
            r'\bt(\d)',  # t1, t2, t3 (solo al inicio de palabra)
            r'\be(\d)',  # e1, e2, e3 (solo al inicio de palabra)
        ]

        for pattern in patterns:
            match = re.search(pattern, file_lower)
            if match:
                criterion_num = int(match.group(1))
                # Validar que sea un n√∫mero v√°lido (1-5)
                if 1 <= criterion_num <= 5:
                    return criterion_num

        return None

    def _get_timestamp(self) -> str:
        """Retorna timestamp actual"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


if __name__ == "__main__":
    # Test del generador de feedback
    print("=== Test GPT Feedback Generator ===\n")

    generator = GPTFeedbackGenerator()

    # Cargar r√∫brica de ejemplo
    rubric_path = "../courses/machine_learning/condiciones.json"
    with open(rubric_path, 'r', encoding='utf-8') as f:
        rubric = json.load(f)

    # Documento de ejemplo
    test_document = """
    # Proyecto de Machine Learning: Predicci√≥n de Precios

    ## Introducci√≥n
    Este proyecto aborda el problema de predecir precios de casas usando regresi√≥n.
    El objetivo es crear un modelo preciso que ayude a estimar valores de propiedades.

    ## Exploraci√≥n de Datos
    El dataset contiene 1000 registros con 15 features.
    Se identificaron 20 valores faltantes y 3 outliers.
    La correlaci√≥n entre √°rea y precio es 0.85.

    ## Modelado
    import pandas as pd
    from sklearn.linear_model import LinearRegression
    from sklearn.ensemble import RandomForestRegressor

    # Modelo entrenado con 80/20 split
    model = RandomForestRegressor(n_estimators=100)
    model.fit(X_train, y_train)

    ## Resultados
    RMSE: 15000
    R¬≤: 0.82
    """

    # Evaluar documento
    result = generator.evaluate_document(test_document, rubric)

    if result['success']:
        print(f"\n‚úì Evaluaci√≥n completada")
        print(f"  - Puntaje total: {result['total_score']}/100")
        print(f"  - Secciones evaluadas: {len(result['section_feedbacks'])}")
        print(f"\n  Feedback general:")
        print(f"  {result['overall_feedback']['summary'][:200]}...")
    else:
        print(f"‚úó Error en evaluaci√≥n")
